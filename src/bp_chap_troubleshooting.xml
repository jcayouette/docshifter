<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
                 type="text/xml" 
                 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
instead of xml:id="bp.chapt.suma3.troubleshooting" better simply
use:       xml:id="bp.troubleshooting"
-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp.chapt.suma3.troubleshooting">
 <title>Troubleshooting</title>
 <sect1 xml:id="bp.chapt.suma3.troubleshooting.registering.cloned.salt.systems">
  <title>Registering Cloned Salt Minions</title>

  <para>
   This chapter provides guidance on registering cloned systems with SUSE
   Manager. This includes both Salt and Traditional clients. For more
   information,
   see
   <link
       xlink:href="https://www.novell.com/support/kb/doc.php?id=7012170"/>.
  </para>

  <procedure>
   <title>Registering a Cloned Salt Minion with SUSE Manager</title>
   <step>
    <para>
     Clone your system (for example using the existing cloning mechanism of
     your favorite Hypervisor)
    </para>
    <note>
     <title>Quick Tips</title>
     <para>
      Each step in this section is performed on the cloned system, this
      procedure does not manipulate the original system, which will still be
      registered to SUSE Manager. The cloned virtual machine should have a
      different UUID from the original (this UUID is generated by your
      hypervisor) or SUSE Manager will overwrite the original system data with
      the new one.
     </para>
    </note>
   </step>
   <step>
    <para>
     Make sure your machines have different hostnames and IP addresses, also
     check that /etc/hosts contains the changes you made and the correct host
     entries.
    </para>
   </step>
   <step>
    <para>
     Stop rhnsd daemon with:
    </para>
<screen># /etc/init.d/rhnsd stop </screen>
    <para>
     or alternatively:
    </para>
<screen># rcrhnsd stop</screen>
   </step>
   <step>
    <para>
     Stop osad with:
    </para>
<screen># /etc/init.d/osad stop</screen>
    <para>
     or alternativly:
    </para>
<screen># rcosad stop</screen>
   </step>
   <step>
    <para>
     Remove the osad authentication configuration file and the systemid with:
    </para>
<screen># rm -f /etc/sysconfig/rhn/{osad-auth.conf,systemid}</screen>
   </step>
  </procedure>

  <para>
   The next step you take will depend on the Operating System of the clone.
  </para>

  <para>
   The following scenario can occur after on-boarding cloned Salt minions. If
   after accepting all cloned minion keys from the onboarding page and you see
   only one minion on the System Overview page, this is likely due to these
   machines being clones of the original and using a duplicate machine-id.
   Perform the following steps to resolve this conflict based on OS.
  </para>

  <procedure>
   <title>SLES 12 Registering Salt Clones:</title>
   <step>
    <para>
     SLES 12: If your machines have the same machine ids then delete the file
     on each minion and recreate it:
    </para>
<screen># rm /etc/machine-id
# rm /var/lib/dbus/machine-id
# dbus-uuidgen --ensure
# systemd-machine-id-setup</screen>
   </step>
  </procedure>

  <procedure>
   <title>SLES 11 Registering Salt Clones:</title>
   <step>
    <para>
     SLES 11: As there is no systemd machine id, generate one from dbus:
    </para>
<screen># rm /var/lib/dbus/machine-id
# dbus-uuidgen --ensure</screen>
   </step>
  </procedure>

  <para>
   If your machines still have the same minion id then delete the minion_id
   file on each minion (FQDN will be used when it is regenerated on minion
   restart):
  </para>

<screen># rm /etc/salt/minion_id</screen>

  <para>
   Finally delete accepted keys from Onboarding page and system profile from
   SUSE Manager, and restart the minion with:
  </para>

<screen># systemctl restart salt-minion</screen>

  <para>
   You should be able to re-register them again, but each minion will use a
   different '/etc/machine-id' and should now be correctly displayed on the
   System Overview page.
  </para>
 </sect1>
 <sect1 xml:id="bp.chapt.suma3.troubleshooting.registering.cloned.traditional.systems">
  <title>Registering Cloned Traditional Systems</title>

  <para>
   This section provides guidance on troubleshooting cloned traditional systems
   registered via bootstrap.
  </para>

  <procedure>
   <title>Registering a Cloned System with SUSE Manager (Traditional Systems)</title>
   <step>
    <para>
     Clone your system (using your favorite hypervisor.)
    </para>
    <note>
     <title>Quick Tips</title>
     <para>
      Each step in this section is performed on the cloned system, this
      procedure does not manipulate the original system, which will still be
      registered to SUSE Manager. The cloned virtual machine should have a
      different UUID from the original (this UUID is generated by your
      hypervisor) or SUSE Manager will overwrite the original system data with
      the new one.
     </para>
    </note>
   </step>
   <step>
    <para>
     Change the Hostname and IP addresses, also make sure /etc/hosts contains
     the changes you made and the correct host entries.
    </para>
   </step>
   <step>
    <para>
     Stop rhnsd daemon with:
    </para>
<screen># /etc/init.d/rhnsd stop </screen>
    <para>
     or alternativly:
    </para>
<screen># rcrhnsd stop</screen>
   </step>
   <step>
    <para>
     Stop osad with:
    </para>
<screen># /etc/init.d/osad stop</screen>
    <para>
     or alternativly:
    </para>
<screen># rcosad stop</screen>
   </step>
   <step>
    <para>
     Remove the osad authentifcation configuration file and the systemid with:
    </para>
<screen># rm -f /etc/sysconfig/rhn/{osad-auth.conf,systemid}</screen>
   </step>
  </procedure>

  <para>
   The next step you take will depend on the Operating System of the clone.
  </para>

  <procedure>
   <title>SLES 12 Registering A Cloned Traditional System:</title>
   <step>
    <para>
     Remove the following credential files:
    </para>
<screen># rm  -f /etc/zypp/credentials.d/{SCCcredentials,NCCcredentials}</screen>
   </step>
   <step>
    <para>
     Re-run the bootstrap script. You should now see the cloned system in SUSE
     Manager without overwriting the system it was cloned from.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>SLES 11 Registering A Cloned Traditional System:</title>
   <step>
    <para>
     Continued from section 1 step 5:
    </para>
<screen># suse_register -E</screen>
    <para>
     (--erase-local-regdata, Erase all local files created from a previous
     executed registration. This option make the system look like never
     registered)
    </para>
   </step>
   <step>
    <para>
     Re-run the bootstrap script. You should now see the cloned system in SUSE
     Manager without overwriting the system it was cloned from.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>SLES 10 Registering A Cloned Traditional System:</title>
   <step>
    <para>
     Continued from section 1 step 5:
    </para>
<screen># rm -rf /etc/{zmd,zypp}</screen>
   </step>
   <step>
<screen># rm -rf /var/lib/zypp/  # ¡¡¡¡¡ except /var/lib/zypp/db/products/ !!!!!</screen>
   </step>
   <step>
<screen># rm -rf /var/lib/zmd/</screen>
   </step>
   <step>
    <para>
     Re-run the bootstrap script. You should now see the cloned system in SUSE
     Manager without overwriting the system it was cloned from.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>RHEL 5,6 and 7</title>
   <step>
    <para>
     Continued from section 1 step 5:
    </para>
<screen># rm  -f /etc/NCCcredentials</screen>
   </step>
   <step>
    <para>
     Re-run the bootstrap script. You should now see the cloned system in SUSE
     Manager without overwriting the system it was cloned from.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="bp.chapt.suma3.troubleshooting.osad.jabberd">
  <title>Typical OSAD/jabberd Challenges</title>

  <para>
   This section provides answers for typical issues regarding OSAD and jabberd.
  </para>

  <sect2>
   <title>Open File Count Exceeded</title>
   <para>
    <literal>SYMPTOMS</literal>: OSAD clients cannot contact the SUSE Manager
    Server, and jabberd requires long periods of time to respond on port 5222.
   </para>
   <para>
    <literal>CAUSE</literal>: The number of maximum files that a jabber user
    can open is lower than the number of connected clients. Each client
    requires one permanently open TCP connection and each connection requires
    one file handler. The result is jabberd begins to queue and refuse
    connections.
   </para>
   <para>
    <literal>CURE</literal>: Edit the
    <filename>/etc/security/limits.conf</filename> to something similar to the
    following: <literal>jabbersoftnofile&lt;#clients + 100&gt;
    jabberhardnofile&lt;#clients + 1000&gt;</literal>
   </para>
   <para>
    This will vary according to your setup. For example in the case of 5000
    clients: <literal>jabbersoftnofile5100 jabberhardnofile6000</literal>
   </para>
   <para>
    Ensure you update the <filename>/etc/jabberd/c2s.xml</filename> max_fds
    parameter as well. For example:
    <literal>&lt;max_fds&gt;6000&lt;/max_fds&gt;</literal>
   </para>
   <para>
    <literal>EXPLANATION</literal>: The soft file limit is the limit of the
    maximum number of open files for a single process. In SUSE Manager the
    highest consuming process is c2s, which opens a connection per client. 100
    additional files are added, here, to accommodate for any non-connection
    file that c2s requires to work correctly. The hard limit applies to all
    processes belonging to the jabber user, and accounts for open files from
    the router, s2s and sm processes additionally.
   </para>
  </sect2>

  <sect2>
   <title>jabberd Database Corruption</title>
   <para>
    <literal>SYMPTOMS</literal>: After a disk is full error or a disk crash
    event, the jabberd database may have become corrupted. jabberd may then
    fail to start during spacewalk-service start:
   </para>
<screen>Starting spacewalk services...
   Initializing jabberd processes...
       Starting router                                                                   done
       Starting sm startproc:  exit status of parent of /usr/bin/sm: 2                   failed
   Terminating jabberd processes...</screen>
   <para>
    /var/log/messages shows more details:
   </para>
<screen>jabberd/sm[31445]: starting up
jabberd/sm[31445]: process id is 31445, written to /var/lib/jabberd/pid/sm.pid
jabberd/sm[31445]: loading 'db' storage module
jabberd/sm[31445]: db: corruption detected! close all jabberd processes and run db_recover
jabberd/router[31437]: shutting down</screen>
   <para>
    <literal>CURE</literal>: Remove the jabberd database and restart. Jabberd
    will automatically re-create the database:
   </para>
<screen> spacewalk-service stop
 rm -Rf /var/lib/jabberd/db/*
 spacewalk-service start</screen>
   <para>
    An alternative approach would be to test another database, but SUSE Manager
    does not deliver drivers for this:
   </para>
<screen> rcosa-dispatcher stop
 rcjabberd stop
 cd /var/lib/jabberd/db
 rm *
 cp /usr/share/doc/packages/jabberd/db-setup.sqlite .
 sqlite3 sqlite.db &lt; db-setup.sqlite
 chown jabber:jabber *
 rcjabberd start
 rcosa-dispatcher start</screen>
  </sect2>

  <sect2>
   <title>Capturing XMPP Network Data for Debugging Purposes</title>
   <para>
    If you are experiencing bugs regarding OSAD, it can be useful to dump
    network messages in order to help with debugging. The following procedures
    provide information on capturing data from both the client and server side.
   </para>
   <procedure>
    <title>Server Side Capture</title>
    <step>
     <para>
      Install the <package>tcpdump</package> package on the SUSE Manager Server
      as root: <command>zypper in tcpdump </command>
     </para>
    </step>
    <step>
     <para>
      Stop the OSA dispatcher and Jabber processes with
      <command>rcosa-dispatcher stop</command> and <command>rcjabberd
      stop.</command>
     </para>
    </step>
    <step>
     <para>
      Start data capture on port 5222: <command>tcpdump -s 0 port 5222 -w
      server_dump.pcap</command>
     </para>
    </step>
    <step>
     <para>
      Start the OSA dispatcher and Jabber processes: <command>rcosa-dispatcher
      start</command> and <command>rcjabberd start</command>.
     </para>
    </step>
    <step>
     <para>
      Open a second terminal and execute the following commands:
      <command>rcosa-dispatcher start</command> and <command>rcjabberd
      start</command>.
     </para>
    </step>
    <step>
     <para>
      Operate the SUSE Manager server and clients so the bug you formerly
      experienced is reproduced.
     </para>
    </step>
    <step>
     <para>
      One you have finished your capture re-open terminal 1 and stop the
      capture of data with: <keycombo> <keycap>CTRL</keycap> <keycap>c</keycap>
      </keycombo>
     </para>
    </step>
   </procedure>
   <procedure>
    <title>Client Side Capture</title>
    <step>
     <para>
      Install the tcpdump package on your client as root: <command>zypper in
      tcpdump</command>
     </para>
    </step>
    <step>
     <para>
      Stop the OSA process: <command>rcosad stop</command>.
     </para>
    </step>
    <step>
     <para>
      Begin data capture on port 5222: <command>tcpdump -s 0 port 5222 -w
      client_client_dump.pcap</command>
     </para>
    </step>
    <step>
     <para>
      Open a second terminal and start the OSA process: <command>rcosad
      start</command>
     </para>
    </step>
    <step>
     <para>
      Operate the SUSE Manager server and clients so the bug you formerly
      experienced is reproduced.
     </para>
    </step>
    <step>
     <para>
      Once you have finished your capture re-open terminal 1 and stop the
      capture of data with: <keycombo> <keycap>CTRL</keycap> <keycap>c</keycap>
      </keycombo>
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2>
   <title>Engineering Notes: Analyzing Captured Data</title>
   <para>
    This section provides information on analyzing the previously captured data
    from client and server.
   </para>
   <procedure>
    <step>
     <para>
      Obtain the certificate file from your SUSE Manager server:
      /etc/pki/spacewalk/jabberd/server.pem
     </para>
    </step>
    <step>
     <para>
      Edit the certificate file removing all lines before <literal>----BEGIN
      RSA PRIVATE KEY-----</literal>, save it as key.pem
     </para>
    </step>
    <step>
     <para>
      Install Wireshark as root with: <command>zypper in wireshark</command>
     </para>
    </step>
    <step>
     <para>
      Open the captured file in wireshark.
     </para>
    </step>
    <step>
     <para>
      From <guimenu>Eidt</guimenu><guimenu>Preferences</guimenu> select SSL
      from the left pane.
     </para>
    </step>
    <step>
     <para>
      Select RSA keys list: <guimenu>Edit</guimenu><guimenu>New</guimenu>
     </para>
     <itemizedlist>
      <listitem>
       <para>
        IP Address any
       </para>
      </listitem>
      <listitem>
       <para>
        Port: 5222
       </para>
      </listitem>
      <listitem>
       <para>
        Protocol: xmpp
       </para>
      </listitem>
      <listitem>
       <para>
        Key File: open the key.pem file previously edited.
       </para>
      </listitem>
      <listitem>
       <para>
        Password: leave blank
       </para>
      </listitem>
     </itemizedlist>
     <para>
      For more information see also:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <link xlink:href="https://wiki.wireshark.org/SSL"/>
       </para>
      </listitem>
      <listitem>
       <para>
        <link
                                    xlink:href="https://bugs.wireshark.org/bugzilla/show_bug.cgi?id=3444"
                                />
       </para>
      </listitem>
     </itemizedlist>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <!-- ============================================================ -->
 <!-- requested 2017-05-05:
      https://bugzilla.suse.com/show_bug.cgi?id=1037798
 -->
 <sect1 xml:id="bp.troubleshooting.timeouts">
  <title>RPC Connection Timeout Settings</title>
  <!--idx-->
  <indexterm>
  <primary>connection timeout</primary></indexterm><indexterm>
  <primary>timeout settings</primary>
  <secondary>RPC connection</secondary>
 </indexterm>

  <para>
   RPC connection timeouts are configurable on the &susemgr; server,
   &susemgrproxy; server, and the clients. For example, if package
   downloads take longer then expected, you can increase timeout values.
   <command>spacewalk-proxy restart</command> should be run after the
   setting is added or modified.
  </para>

  <para>
   Set the following variables to a value in seconds specifying how long an
   RPC connection may take at maximum:
  </para>

<!--
- Comment #1 from Michael Calmer <mc@suse.com> 2013-08-08 08:33:38 UTC -
You can configure the timeout (how long a RPC connection maximal take) now on
Server, Proxy and Client.
  -->

  <variablelist>
   <varlistentry>
    <term>Server &mdash; <filename>/etc/rhn/rhn.conf</filename>:</term>
    <listitem>
<screen>server.timeout = <replaceable>number</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Proxy Server &mdash; <filename>/etc/rhn/rhn.conf</filename>:</term>
    <listitem>
<!-- bsc#929379 -->
<screen>proxy.timeout = <replaceable>number</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&sls; Clients (using <package>zypp-plugin-spacewalk</package>) &mdash; <filename>/etc/zypp/zypp.conf</filename>:</term>
    <listitem>
<screen>## Valid values:  [0,3600]
## Default value: 180
download.transfer_timeout = 180</screen>
     <para>
      This is the maximum time in seconds that a transfer operation is
      allowed to take. This is useful for preventing batch jobs from hanging
      for hours due to slow networks or links going down. If limiting
      operations to less than a few minutes, you risk aborting perfectly
      normal operations.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&rhel; Clients (using <package>yum-rhn-plugin</package>) &mdash; <filename>/etc/yum.conf</filename>:</term>
    <listitem>
<screen>timeout = <replaceable>number</replaceable></screen>
<!--
(yum has multiple places where you can configure something and I am not sure
where it read all the timeout from. Possible is also the special 
/etc/yum/pluginconf.d/rhnplugin.conf and per repo in /etc/yum.repos.d/...repo )
     -->
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
<!-- ============================================================ -->

</chapter>
